# Отчет по лабораторной работе "Генерация последовательностей"

### Жуков Максим Александрович, М8О-307Б-17
Номер в группе: 13, Вариант: 1

### Цель работы

Научиться генерировать последовательности с помощью рекуррентных нейронных сетей.

### Используемые входные данные

Использовался текст с  http://lib.ru/RUSS_DETEKTIW/BAKUNIN/chaika.txt


### Предварительная обработка входных данных

Описана в ipynb файле.


### Эксперимент 1: RNN

#### Архитектура сети

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/rnn.png)

#### Результат

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/rnn1.png)

#### Вывод по данному эксперименту

В сгенерированном тексте прослеживются правильные слова (однако есть и неразбериха), знаки припенания и предлоги. Также прослеживаются части предложений, однако для дальнейших выводов надо просмотреть остальные выводы моделей.

### Эксперимент 2: Однослойная LSTM

#### Архитектура сети

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/LSTM.png)

#### Результат

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/LSTM1.png)

#### Вывод по данному эксперименту

Обратим внимание что, потеря данных снизилась, а сам текст стал осмысленнее. Начали проявляться вставки базовых конструкций (сущ. + гл.).

### Эксперимент 3: Двуслойная LSTM

#### Архитектура сети

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/LSTM2.png)

#### Результат

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/LSTM22.png)

#### Вывод по данному эксперименту

На 10 эпохе потери составили такое же значение, что на однойслойной LSTM при 20, что позволяет предположить, что на 20 выигрыш был бы в районе 1.0 - 1.2. Также в части текста использовалось несколько временных конструкций, что еще раз подтверждает улучшений модели.

### Эксперимент 4: GRU

#### Архитектура сети

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/GPU.png)

#### Результат

![](https://github.com/ZhukovMA/LAB_TextGen/blob/main/Images/GPU1.png)

#### Вывод по данному эксперименту

Функция потерь показывает наименьшее значение (несчитая двуслойной LSTM), что показыает преимушество этой модели, хотя до полностью осмысленного текста еще далеко.

### Выводы

Таким образом, было создано 4 модели. Наилучшее значение было показано на двухслойной LSTM и GRU. Хоть все текста могли генерировать случайный текст, однако этот текст был далек от осмысленного, но адекватные участки генерации прослеживались.

Такой результат мог получиться из-за "мусора" в обучающем тексте, может если очистить текст, то можно было бы получить точнее результаты. Также стоило бы поэксперементировать с размером batch_size и количеством epochs, однако процесс обучения занимает слишком много времени. Как вариант был бы увеличение числа epochs, но все опять упирается во время обучения.
